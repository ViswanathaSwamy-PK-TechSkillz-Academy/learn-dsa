# Understanding Algorithmic Complexity

## 1. Introduction to Algorithmic Complexity

### 1.1. What is Algorithmic Complexity?

> 1. **Definition:** Algorithmic complexity refers to the measure of how the computational resources, such as time and space, required by an algorithm grow as the size of the input increases.
> 1. **Purpose:** It helps in analyzing and comparing algorithms based on their efficiency and performance characteristics.
> 1. **Examples:** Time complexity, space complexity, and their various notations (Big O, Big Omega, Big Theta) are commonly used metrics to evaluate algorithmic complexity.

### 1.2. Algorithmic Complexity Notations

#### 1.2.1. Big Omega (Ω)

> 1. **Definition:** Big Omega notation represents the lower bound on the running time or space complexity of an algorithm.
> 1. **Purpose:** It provides information about the best-case performance of an algorithm.
> 1. **Example:** If we say an algorithm has a time complexity of Ω(n), it means that the best-case running time grows linearly with the input size.
> 1. **Use Case:** Big Omega is useful for understanding lower bounds and best-case scenarios.

#### 1.2.2. Big Theta (Θ)

> 1. **Definition:** Big Theta notation represents both the upper and lower bounds on the running time or space complexity of an algorithm.
> 1. **Purpose:** It describes the tightest possible bound for an algorithm’s performance.
> 1. **Example:** If we say an algorithm has a time complexity of Θ(n), it means that both the best-case and worst-case running times grow linearly with the input size.
> 1. **Use Case:** Big Theta is commonly used when we want to express a precise estimate of an algorithm’s behavior.

#### 1.2.3. Big O (O)

> 1. **Definition:** Big O notation represents the upper bound on the running time or space complexity of an algorithm.
> 1. **Purpose:** It helps us understand how an algorithm’s performance scales as the input size grows.
> 1. **Example:** If we say an algorithm has a time complexity of O(n), it means that the worst-case running time grows linearly with the input size.
> 1. **Use Case:** Big O is often used to analyze the worst-case scenario.

## 2. Understanding Big O Notation

### 2.1. Big O (O) Basic Concepts

> 1. **Definition:** Big O notation represents the upper bound on the running time or space complexity of an algorithm.
> 1. **Purpose:** It helps us understand how an algorithm’s performance scales as the input size grows.
> 1. **Example:** If we say an algorithm has a time complexity of O(n), it means that the worst-case running time grows linearly with the input size.
Use Case: Big O is often used to analyze the worst-case scenario.

### 2.2. Common Time Complexities

#### 2.2.1. O(1): Constant Time

> 1. **Description:** Algorithms with constant time complexity execute in a fixed amount of time, regardless of the input size. They are highly efficient.
> 1. **Example:** Accessing an array element by its index.

**Sample codes:**

```python
# Python
def access_element(arr, index):
    return arr[index]
```

```javascript
// JavaScript
function accessElement(arr, index) {
    return arr[index];
}
```

#### 2.2.2. O(log n): Logarithmic Time

> 1. **Description:** Logarithmic time complexity indicates that the
divides the problem into smaller parts in each step (`divide and conquer`). It's efficient for large datasets.
> 1. **Example:** Binary search.

**Sample codes:**

```python
# Python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = left + (right - left) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
```

```javascript
// JavaScript
function binarySearch(arr, target) {
    let left = 0;
    let right = arr.length - 1;
    while (left <= right) {
        const mid = Math.floor((left + right) / 2);
        if (arr[mid] === target) {
            return mid;
        } else if (arr[mid] < target) {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }
    return -1;
}
```

#### 2.2.3. O(n): Linear Time

> 1. **Description:** Linear time complexity means the running time grows linearly with the input size. It's `proportional` to the data set size.
> 1. **Example:** Looping through an array.

**Sample codes:**

```python
# Python
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1
```

```javascript
// JavaScript
function linearSearch(arr, target) {
    for (let i = 0; i < arr.length; i++) {
        if (arr[i] === target) {
            return i;
        }
    }
    return -1;
}
```

#### 2.2.4. O(n log n): Linearithmic Time

> 1. **Description:** Linearithmic time complexity combines linear and logarithmic behavior. It's common in sorting and searching algorithms.
> 1. **Example:** Merge sort, quick sort.

**Sample codes:**

```python
# Python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left_half = merge_sort(arr[:mid])
    right_half = merge_sort(arr[mid:])
    return merge(left_half, right_half)


def merge(left, right):
    result = []
    i, j = 0, 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result
```

```javascript
// JavaScript
function mergeSort(arr) {
    if (arr.length <= 1) {
        return arr;
    }
    const mid = Math.floor(arr.length / 2);
    const leftHalf = mergeSort(arr.slice(0, mid));
    const rightHalf = mergeSort(arr.slice(mid));

    return merge(leftHalf, rightHalf);
}

function merge(left, right) {
    const result = [];
    let i = 0;
    let j = 0;
    while (i < left.length && j < right.length) {
        if (left[i] < right[j]) {
            result.push(left[i]);
            i++;
        } else {
            result.push(right[j]);
            j++;
        }
    }
    
    return result.concat(left.slice(i), right.slice(j));
}
```

#### 2.2.5. O(n^2): Polynomial Time

> 1. **Description:** Polynomial time complexity involves `nested loops` for each power of n. It's less efficient for large datasets.
> 1. **Example:** Bubble sort (O(n^2)).

**Sample codes:**

```python
# Python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
```

```javascript
// JavaScript
function bubbleSort(arr) {
    const n = arr.length;
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < n - i - 1; j++) {
            if (arr[j] > arr[j + 1]) {
                [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]];
            }
        }
    }
}
```

## 3. Simplifying Big O Notation: Rules and Examples

### 3.1. Drop Constants

> 1. In Big O notation, we ignore constant factors. Whether an operation takes 1 unit of time or 1000 units of time, it’s still considered constant time.
> 1. For example, if we have an expression like O(2n), we simplify it to O(n). Similarly, O(500) simplifies to O(1).

### 3.2. Drop Non-Dominants

> 1. When we have multiple terms in an expression, we focus on the dominant term—the one that grows the fastest as the input size increases.
> 1. For instance, if we have O(n + 10), we simplify it to O(n). The constant 10 becomes insignificant compared to the linear term.
Similarly, O(1000n + 50) simplifies to O(n).

### 3.3. Different Terms for Input

> 1. If we have an expression with different terms based on the input size (n), we simplify it by considering only the dominant term.
> 1. For example, O(n^2 + 5n + 8) simplifies to O(n^2). The smaller terms (5n + 8) are negligible compared to the quadratic term.

---

Simplifying Big O notation involves following certain rules to express the time or space complexity of an algorithm more concisely. Here are some common rules for simplifying Big O notation:

1. **Drop Constants**: Ignore constant factors when determining the Big O notation. For example, if an algorithm has a time complexity of \( 2n^2 + 3n + 5 \), it can be simplified to \( O(n^2) \) by dropping the constants \( 2 \) and \( 3 \).

2. **Keep Dominant Terms**: Focus on the term with the highest growth rate as the input size increases. For instance, if an algorithm has time complexities \( O(n^2) \) and \( O(n) \), choose \( O(n^2) \) because it grows faster.

3. **Additive Rule**: For algorithms with multiple steps or loops, add the complexities together. For example, if an algorithm has two nested loops, one with \( O(n) \) complexity and the other with \( O(m) \) complexity, the overall complexity is \( O(n + m) \).

4. **Multiplicative Rule**: For algorithms with nested operations, multiply the complexities together. For example, if an algorithm has nested loops, one with \( O(n) \) complexity and the other with \( O(m) \) complexity, the overall complexity is \( O(n \times m) \).

5. **Drop Non-Dominant Terms**: When simplifying, drop lower-order terms and constants. For example, if an algorithm has a time complexity of \( O(n^2 + n) \), drop the \( n \) term and simplify it to \( O(n^2) \).

6. **Drop Lower Order Terms**: If an algorithm has multiple terms with different orders of magnitude, drop the lower-order terms. For example, if an algorithm has a complexity of \( O(n^3 + n^2 + n) \), simplify it to \( O(n^3) \).

These rules help in simplifying the expression of the time or space complexity of algorithms using Big O notation, making it easier to compare and analyze different algorithms.

Certainly! Here are JavaScript sample codes for each of the rules of Big O simplification:

### 1. Drop Constants

```javascript
// Constant factors are ignored
function exampleFunction(n) {
    for (let i = 0; i < 2 * n; i++) {
        console.log(i);
    }
}
// Time complexity: O(n)
```

### 2. Keep Dominant Terms

```javascript
// Focus on the term with the highest growth rate
function exampleFunction(n, m) {
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < m; j++) {
            console.log(i, j);
        }
    }
}
// Time complexity: O(n * m)
```

### 3. Additive Rule

```javascript
// Add complexities together for multiple steps or loops
function exampleFunction(n, m) {
    for (let i = 0; i < n; i++) {
        console.log(i);
    }
    for (let j = 0; j < m; j++) {
        console.log(j);
    }
}
// Time complexity: O(n + m)
```

### 4. Multiplicative Rule

```javascript
// Multiply complexities for nested operations
function exampleFunction(n, m) {
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < m; j++) {
            console.log(i, j);
        }
    }
}
// Time complexity: O(n * m)
```

### 5. Drop Non-Dominant Terms

```javascript
// Drop lower-order terms and constants
function exampleFunction(n) {
    for (let i = 0; i < n * n; i++) {
        console.log(i);
    }
    for (let j = 0; j < n; j++) {
        console.log(j);
    }
}
// Time complexity: O(n^2)
```

### 6. Drop Lower Order Terms

```javascript
// Drop lower-order terms
function exampleFunction(n) {
    for (let i = 0; i < n * n * n; i++) {
        console.log(i);
    }
    for (let j = 0; j < n * n; j++) {
        console.log(j);
    }
    for (let k = 0; k < n; k++) {
        console.log(k);
    }
}
// Time complexity: O(n^3)
```

These examples demonstrate how each rule applies to different scenarios, helping to simplify the expression of time complexity using Big O notation in JavaScript.
